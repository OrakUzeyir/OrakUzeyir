{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WfScOSYa_on"
      },
      "outputs": [],
      "source": [
        "!pip install contractions\n",
        "!pip install nltk\n",
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CvoFJG9FfLE"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOLRY4_oAWEg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#import googletrans\n",
        "#from googletrans import Translator\n",
        "\n",
        "# yazim degistirme you've => you have\n",
        "import contractions\n",
        "# regex\n",
        "import re\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"all\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_VLxDcrFqm6"
      },
      "source": [
        "**Create Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2QSD8jUF6li"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/complaint_analysis_nlp_caretta/consumer_complaints.csv')\n",
        "\n",
        "dataset.drop([\"date_received\", \"sub_product\", \"issue\", \"sub_issue\", \"company_public_response\",\n",
        "              \"company\", \"state\", \"zipcode\", \"tags\", \"consumer_consent_provided\", \"submitted_via\",\n",
        "              \"date_sent_to_company\", \"company_response_to_consumer\", \"timely_response\", \"consumer_disputed?\",\n",
        "              \"complaint_id\"], axis = 1, inplace = True)\n",
        "\n",
        "dataset.dropna(inplace=True)\n",
        "\n",
        "dataset.drop_duplicates(keep = False, inplace = True)\n",
        "\n",
        "print(dataset[dataset['product'] == \"Debt collection\"].count())\n",
        "print(dataset[dataset['product'] == \"Mortgage\"].count())\n",
        "print(dataset[dataset['product'] == \"Credit card\"].count())\n",
        "print(dataset[dataset['product'] == \"Credit reporting\"].count())\n",
        "print(dataset[dataset['product'] == \"Bank account or service\"].count())\n",
        "\n",
        "print(dataset.info())\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO-8me3yGg46"
      },
      "source": [
        "**Translate Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f67SlVeWsZ7f"
      },
      "outputs": [],
      "source": [
        "def translator(text, sourceLanguage, targetLanguage):\n",
        "  translator = Translator()\n",
        "  result = translator.translate(text, src=sourceLanguage, dest=targetLanguage)\n",
        "\n",
        "  return result.text\n",
        "\n",
        "def translatedCSV(data):\n",
        "  size = len(data)\n",
        "\n",
        "  for i in range(0,size):\n",
        "    dataset[\"consumer_complaint_narrative\"].iloc[i] = translator(dataset[\"consumer_complaint_narrative\"].iloc[i], \"en\", \"tr\")\n",
        "\n",
        "    #print(dataset[\"consumer_complaint_narrative\"].iloc[i])\n",
        "    #print(\"\\n\")\n",
        "\n",
        "# not using\n",
        "#translatedCSV(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-3FRMuHDp94"
      },
      "source": [
        "**Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxu1PYVu-hCm"
      },
      "outputs": [],
      "source": [
        "# product distribution show\n",
        "#fig,ax = plt.subplots(figsize=(18,6))\n",
        "#sns.countplot(x='product',data=dataset)\n",
        "\n",
        "drop_class_index_list = [dataset[dataset[\"product\"] == \"Consumer Loan\"].index,\n",
        "                         dataset[dataset[\"product\"] == \"Student loan\"].index,\n",
        "                         dataset[dataset[\"product\"] == \"Payday loan\"].index,\n",
        "                         dataset[dataset[\"product\"] == \"Money transfers\"].index,\n",
        "                         dataset[dataset[\"product\"] == \"Other financial service\"].index,\n",
        "                         dataset[dataset[\"product\"] == \"Prepaid card\"].index]\n",
        "\n",
        "for drop_class_index in drop_class_index_list:\n",
        "  dataset.drop(drop_class_index, axis=0, inplace=True)\n",
        "\n",
        "# product distribution show\n",
        "#fig,ax = plt.subplots(figsize=(18,6))\n",
        "#sns.countplot(x='product',data=dataset)\n",
        "\n",
        "# dataset index reset\n",
        "#dataset = dataset.reset_index()\n",
        "\n",
        "# convert to lowercase\n",
        "#dataset['consumer_complaint_narrative'] = dataset['consumer_complaint_narrative'].apply(lambda x: ' '.join([i.lower() for i in x.split()]))\n",
        "\n",
        "# expand constractions\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "\n",
        "    return phrase\n",
        "\n",
        "def decontracted_dataset(data,column_name):\n",
        "  for i in data[column_name]:\n",
        "    dataset[column_name] = dataset[column_name].replace([i],decontracted(i))\n",
        "\n",
        "# not working expand constractions\n",
        "#decontracted_dataset(dataset,\"consumer_complaint_narrative\")\n",
        "#print(dataset.head(100).to_string())\n",
        "\n",
        "# feature selection\n",
        "#dataset.drop([\"level_0\",\"index\"], axis = 1, inplace = True)\n",
        "\n",
        "# Correcting Spelling\n",
        "#dataset['consumer_complaint_narrative'] = dataset['consumer_complaint_narrative'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "\n",
        "print(dataset.head(100).to_string())\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "def lemmatization(sentence):\n",
        "    sent = TextBlob(sentence)\n",
        "    tag_dict = {\"J\": 'a', \n",
        "                \"N\": 'n', \n",
        "                \"V\": 'v', \n",
        "                \"R\": 'r'}\n",
        "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
        "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
        "    return \" \".join(lemmatized_list)\n",
        "\n",
        "# Lemmatization (köklerine indirgeme)##\n",
        "#dataset[\"consumer_complaint_narrative\"] = dataset[\"consumer_complaint_narrative\"].apply(lemmatization)\n",
        "\n",
        "def preprocess(df, column):\n",
        "    df[column] = df[column].str.lower()  # converting to lower case\n",
        "\n",
        "    decontracted_dataset(df, column) # expand constractions\n",
        "\n",
        "    stop = stopwords.words('english')\n",
        "    df[column] = df[column].apply(lambda x: ' '.join([i for i in x.split() if i not in stop])) # drop stopwords\n",
        "\n",
        "    df[column] = df[column].str.replace('/^#\\w+$/', '')  # removing hashtags\n",
        "    df[column] = df[column].str.replace('[^\\w\\s]', '') # removing punctuation\n",
        "    df[column] = df[column].str.replace('[_]', '') \n",
        "    df[column] = df[column].str.replace('\\d', '')  # removing numbers\n",
        "    df[column] = df[column].str.replace('\\s\\s+', ' ')  # converting multiple spaces to single space\n",
        "\n",
        "    #df[column] = df[column].apply(lambda x: str(TextBlob(x).correct())) # Correcting Spelling (yazım düzeltme)\n",
        "\n",
        "    df[column] = df[column].apply(lemmatization) # Lemmatization (köklerine indirgeme)\n",
        "\n",
        "    return df\n",
        "\n",
        "dataset = preprocess(dataset, \"consumer_complaint_narrative\")\n",
        "print(dataset.head(100).to_string())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fx_w5uDWK-p"
      },
      "source": [
        "**Oversampling-Undersampling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBzYPYcsM41F",
        "outputId": "760c6d24-d663-4f5b-8e78-dc52c09513f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verilerin Dağılımı : \n",
            "\n",
            "Debt collection            16860\n",
            "Mortgage                   14902\n",
            "Credit reporting           11511\n",
            "Credit card                 7895\n",
            "Bank account or service     5671\n",
            "Name: product, dtype: int64\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Oversampling ve Undersampling İşlemleri Sonrası Verilerin Dağılımı : \n",
            "\n",
            "Mortgage                   10000\n",
            "Credit reporting           10000\n",
            "Debt collection            10000\n",
            "Credit card                 8500\n",
            "Bank account or service     6500\n",
            "Name: product, dtype: int64\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "x shape:  (45000,)\n",
            "y shape:  (45000,)\n",
            "X_train:  (33750,)\n",
            "X_test:  (11250,)\n",
            "y_train:  (33750,)\n",
            "y_test:  (11250,)\n",
            "519175    Bank account or service\n",
            "209052                   Mortgage\n",
            "255366                   Mortgage\n",
            "240178                   Mortgage\n",
            "527932                   Mortgage\n",
            "                   ...           \n",
            "290829                Credit card\n",
            "490619                   Mortgage\n",
            "297336                Credit card\n",
            "493414            Debt collection\n",
            "506912                Credit card\n",
            "Name: product, Length: 33750, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# for oversampling and undersampling\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# for train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# verilerin dağılımı\n",
        "print(\"\\nVerilerin Dağılımı : \\n\")\n",
        "index = pd.Index(dataset[\"product\"])\n",
        "print(index.value_counts())\n",
        "print(\"\\n\\n\\n\\n\")\n",
        "\n",
        "#veri seti dengeleştirme\n",
        "\n",
        "# sentetik veri ekleme\n",
        "def oversampling(minority, majority, is_replace, random_state):\n",
        "  return resample(minority, n_samples=majority, replace=is_replace, random_state=random_state) \n",
        "\n",
        "# veri azaltma\n",
        "def undersampling(majority, minority, is_replace, random_state):\n",
        "  return resample(majority, n_samples=minority, replace=is_replace, random_state=random_state)\n",
        "\n",
        "dataset_debt_collection = dataset[dataset[\"product\"] == \"Debt collection\"]\n",
        "dataset_mortgage = dataset[dataset[\"product\"] == \"Mortgage\"]\n",
        "dataset_credit_reporting = dataset[dataset[\"product\"] == \"Credit reporting\"]\n",
        "dataset_credit_card = dataset[dataset[\"product\"] == \"Credit card\"]\n",
        "dataset_bank_account_or_service = dataset[dataset[\"product\"] == \"Bank account or service\"]\n",
        "\n",
        "# using undersampling\n",
        "dataset_debt_collection_downsampled = undersampling(dataset_debt_collection, 10000, False, 1234)\n",
        "dataset_mortgage_downsampled = undersampling(dataset_mortgage, 10000, False, 1234)\n",
        "dataset_credit_reporting_downsampled = undersampling(dataset_credit_reporting, 10000, False, 1234)\n",
        "\n",
        "# using oversampling\n",
        "dataset_credit_card_upsampled = oversampling(dataset_credit_card, 8500, True, 1234)\n",
        "dataset_bank_account_or_service_upsampled = oversampling(dataset_bank_account_or_service, 6500, True, 1234)\n",
        "\n",
        "# concat undersampling dataset and oversampling dataset\n",
        "dataset_upsampled_downsampled = pd.concat([dataset_debt_collection_downsampled, dataset_mortgage_downsampled, \n",
        "                                dataset_credit_reporting_downsampled, dataset_credit_card_upsampled,\n",
        "                                dataset_bank_account_or_service_upsampled], axis=0)\n",
        "\n",
        "#shuffle\n",
        "dataset_upsampled_downsampled = dataset_upsampled_downsampled.sample(frac=1)\n",
        "\n",
        "# oversampling ve undersampling işlemleri sonrası verilerin dağılımı\n",
        "print(\"\\nOversampling ve Undersampling İşlemleri Sonrası Verilerin Dağılımı : \\n\")\n",
        "index = pd.Index(dataset_upsampled_downsampled[\"product\"])\n",
        "print(index.value_counts())\n",
        "print(\"\\n\\n\\n\\n\")\n",
        "\n",
        "# split x and y\n",
        "y = dataset_upsampled_downsampled[\"product\"]\n",
        "x = dataset_upsampled_downsampled[\"consumer_complaint_narrative\"]\n",
        "\n",
        "print(\"x shape: \", x.shape)\n",
        "print(\"y shape: \", y.shape)\n",
        "\n",
        "# split train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)\n",
        "\n",
        "print(\"X_train: \", X_train.shape)\n",
        "print(\"X_test: \", X_test.shape)\n",
        "print(\"y_train: \", y_train.shape)\n",
        "print(\"y_test: \", y_test.shape)\n",
        "print(y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLR0YvC4guo5"
      },
      "source": [
        "**Label Encoding and TF-IDF Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVR53gU7X5Sh",
        "outputId": "30646ebe-2d8d-4d07-ff66-073a8aaea990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_tfidf:   (0, 4952)\t0.06809310560609141\n",
            "  (0, 4943)\t0.09247139105468151\n",
            "  (0, 4913)\t0.0922814934484676\n",
            "  (0, 4849)\t0.08715790250187638\n",
            "  (0, 4818)\t0.16949298149086045\n",
            "  (0, 4756)\t0.182606320665604\n",
            "  (0, 4067)\t0.14341203779594855\n",
            "  (0, 3719)\t0.17267687232976675\n",
            "  (0, 3506)\t0.1121749886490924\n",
            "  (0, 3361)\t0.11884103746830554\n",
            "  (0, 3258)\t0.3396358203811175\n",
            "  (0, 3018)\t0.08324931916324199\n",
            "  (0, 2897)\t0.08996135833135899\n",
            "  (0, 2848)\t0.17531668412593018\n",
            "  (0, 2528)\t0.5603506941260252\n",
            "  (0, 2406)\t0.09260888925920012\n",
            "  (0, 2367)\t0.0904406458387793\n",
            "  (0, 2299)\t0.16383409752440944\n",
            "  (0, 2046)\t0.11914455819963152\n",
            "  (0, 1905)\t0.0758423948353096\n",
            "  (0, 1669)\t0.13520016677422897\n",
            "  (0, 1273)\t0.15986236950990798\n",
            "  (0, 1030)\t0.2394270919764234\n",
            "  (0, 926)\t0.07208637660463477\n",
            "  (0, 775)\t0.17611712409044375\n",
            "  :\t:\n",
            "  (33749, 1280)\t0.55210118340925\n",
            "  (33749, 1181)\t0.1824960498426305\n",
            "  (33749, 1101)\t0.19273125617133505\n",
            "  (33749, 1030)\t0.04057836343710549\n",
            "  (33749, 999)\t0.08417694422421247\n",
            "  (33749, 995)\t0.03238318661913356\n",
            "  (33749, 980)\t0.042431550970276656\n",
            "  (33749, 926)\t0.12217277353955289\n",
            "  (33749, 911)\t0.08776321053863129\n",
            "  (33749, 844)\t0.03762051502254806\n",
            "  (33749, 801)\t0.2505243272463631\n",
            "  (33749, 632)\t0.08890220519961492\n",
            "  (33749, 608)\t0.11689905125379743\n",
            "  (33749, 506)\t0.08091550297201199\n",
            "  (33749, 475)\t0.04360382508456574\n",
            "  (33749, 472)\t0.05596823487473826\n",
            "  (33749, 455)\t0.048128113766964284\n",
            "  (33749, 392)\t0.06608060675809209\n",
            "  (33749, 369)\t0.09454178415714046\n",
            "  (33749, 346)\t0.11815851545934139\n",
            "  (33749, 190)\t0.030735306397079134\n",
            "  (33749, 149)\t0.05007654575028462\n",
            "  (33749, 116)\t0.04766952099178089\n",
            "  (33749, 115)\t0.07701771440423497\n",
            "  (33749, 41)\t0.08596419446174562\n",
            "X_test_tfidf:   (0, 4980)\t0.10269659981556042\n",
            "  (0, 4952)\t0.5639260701970175\n",
            "  (0, 4938)\t0.028894009807418092\n",
            "  (0, 4885)\t0.06254217487636682\n",
            "  (0, 4831)\t0.10341358694016015\n",
            "  (0, 4769)\t0.08429769323962577\n",
            "  (0, 4642)\t0.10998100385891053\n",
            "  (0, 4578)\t0.13520034209469842\n",
            "  (0, 4541)\t0.0816064928955067\n",
            "  (0, 4409)\t0.045956581878766674\n",
            "  (0, 4288)\t0.08478001934752068\n",
            "  (0, 4231)\t0.0794717518575825\n",
            "  (0, 4150)\t0.06800208687157196\n",
            "  (0, 3989)\t0.07462905827371547\n",
            "  (0, 3986)\t0.07277638359329898\n",
            "  (0, 3957)\t0.20068803361823737\n",
            "  (0, 3932)\t0.10927250818040223\n",
            "  (0, 3737)\t0.07231319684078982\n",
            "  (0, 3704)\t0.1612254855647961\n",
            "  (0, 3418)\t0.21122235730940309\n",
            "  (0, 3375)\t0.09967903764808728\n",
            "  (0, 3217)\t0.03764307202045716\n",
            "  (0, 3160)\t0.031256234893597086\n",
            "  (0, 3152)\t0.028349104299518854\n",
            "  (0, 3011)\t0.11220136363028065\n",
            "  :\t:\n",
            "  (11249, 1925)\t0.0742830634724685\n",
            "  (11249, 1885)\t0.08941652017455036\n",
            "  (11249, 1695)\t0.301407123332732\n",
            "  (11249, 1573)\t0.07563823445905553\n",
            "  (11249, 1475)\t0.09769329661563757\n",
            "  (11249, 1206)\t0.06483414745917701\n",
            "  (11249, 1101)\t0.11296886093108323\n",
            "  (11249, 1034)\t0.11468212925948401\n",
            "  (11249, 943)\t0.10665452809491519\n",
            "  (11249, 941)\t0.0458215714003229\n",
            "  (11249, 801)\t0.04195545718762479\n",
            "  (11249, 798)\t0.10552476732781851\n",
            "  (11249, 789)\t0.07703568295879071\n",
            "  (11249, 746)\t0.08177896953782135\n",
            "  (11249, 722)\t0.0761197645053507\n",
            "  (11249, 676)\t0.0742830634724685\n",
            "  (11249, 666)\t0.07566786434371447\n",
            "  (11249, 608)\t0.054816029019500306\n",
            "  (11249, 596)\t0.08077334429528704\n",
            "  (11249, 484)\t0.07342005462510401\n",
            "  (11249, 339)\t0.06673295936689452\n",
            "  (11249, 229)\t0.043837930982188765\n",
            "  (11249, 140)\t0.08905067658211946\n",
            "  (11249, 68)\t0.2824938313389102\n",
            "  (11249, 38)\t0.062379154384707196\n"
          ]
        }
      ],
      "source": [
        "from sklearn import model_selection, preprocessing, metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# label encoding for product column\n",
        "enc = preprocessing.LabelEncoder()\n",
        "y_train_label_encoder = enc.fit_transform(y_train)\n",
        "y_test_label_encoder = enc.fit_transform(y_test)\n",
        "\n",
        "# tf-idf for x_train and x_test\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "#tfidf_vect.fit(dataset['consumer_complaint_narrative'])\n",
        "\n",
        "#X_train_tfidf =  tfidf_vect.fit_transform(X_train)\n",
        "#X_test_tfidf =  tfidf_vect.fit_transform(X_test)\n",
        "\n",
        "fitted_vectorizer = tfidf_vect.fit(X_train)\n",
        "X_train_tfidf = fitted_vectorizer.transform(X_train)\n",
        "X_test_tfidf = fitted_vectorizer.transform(X_test)\n",
        "\n",
        "print(\"X_train_tfidf:\", X_train_tfidf)\n",
        "print(\"X_test_tfidf:\", X_test_tfidf)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwe2ASr3g8AJ"
      },
      "source": [
        "**Machine Learning Algorithm's**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDa0naO-g5zW",
        "outputId": "6d76b2c8-df19-4d09-a823-945bdf8815d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression test accuracy: \n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Bank account or service       0.91      0.88      0.89      1599\n",
            "            Credit card       0.89      0.89      0.89      2201\n",
            "       Credit reporting       0.88      0.87      0.88      2463\n",
            "        Debt collection       0.86      0.88      0.87      2520\n",
            "               Mortgage       0.94      0.95      0.94      2467\n",
            "\n",
            "               accuracy                           0.89     11250\n",
            "              macro avg       0.89      0.89      0.89     11250\n",
            "           weighted avg       0.89      0.89      0.89     11250\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "SVM Test accuracy: \n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Bank account or service       0.93      0.91      0.92      1599\n",
            "            Credit card       0.91      0.90      0.91      2201\n",
            "       Credit reporting       0.88      0.89      0.88      2463\n",
            "        Debt collection       0.88      0.89      0.89      2520\n",
            "               Mortgage       0.95      0.95      0.95      2467\n",
            "\n",
            "               accuracy                           0.91     11250\n",
            "              macro avg       0.91      0.91      0.91     11250\n",
            "           weighted avg       0.91      0.91      0.91     11250\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "XGBoost Test accuracy: \n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Bank account or service       0.92      0.93      0.93      1599\n",
            "            Credit card       0.91      0.92      0.91      2201\n",
            "       Credit reporting       0.89      0.88      0.88      2463\n",
            "        Debt collection       0.87      0.88      0.87      2520\n",
            "               Mortgage       0.95      0.94      0.94      2467\n",
            "\n",
            "               accuracy                           0.91     11250\n",
            "              macro avg       0.91      0.91      0.91     11250\n",
            "           weighted avg       0.91      0.91      0.91     11250\n",
            "\n",
            "Naive Bayes test accuracy: \n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Bank account or service       0.90      0.80      0.85      1599\n",
            "            Credit card       0.83      0.82      0.83      2201\n",
            "       Credit reporting       0.80      0.86      0.83      2463\n",
            "        Debt collection       0.84      0.83      0.84      2520\n",
            "               Mortgage       0.92      0.94      0.93      2467\n",
            "\n",
            "               accuracy                           0.86     11250\n",
            "              macro avg       0.86      0.85      0.85     11250\n",
            "           weighted avg       0.86      0.86      0.86     11250\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Random Forest test accuracy: \n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Bank account or service       0.94      0.92      0.93      1599\n",
            "            Credit card       0.93      0.90      0.91      2201\n",
            "       Credit reporting       0.85      0.88      0.86      2463\n",
            "        Debt collection       0.87      0.86      0.86      2520\n",
            "               Mortgage       0.93      0.94      0.93      2467\n",
            "\n",
            "               accuracy                           0.90     11250\n",
            "              macro avg       0.90      0.90      0.90     11250\n",
            "           weighted avg       0.90      0.90      0.90     11250\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Decision Tree test accuracy: \n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Bank account or service       0.84      0.88      0.86      1599\n",
            "            Credit card       0.82      0.84      0.83      2201\n",
            "       Credit reporting       0.79      0.80      0.80      2463\n",
            "        Debt collection       0.79      0.76      0.77      2520\n",
            "               Mortgage       0.89      0.87      0.88      2467\n",
            "\n",
            "               accuracy                           0.82     11250\n",
            "              macro avg       0.83      0.83      0.83     11250\n",
            "           weighted avg       0.82      0.82      0.82     11250\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gaussian Naive Bayes: \n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Bank account or service       0.58      0.78      0.67      1599\n",
            "            Credit card       0.69      0.60      0.64      2201\n",
            "       Credit reporting       0.59      0.82      0.69      2463\n",
            "        Debt collection       0.75      0.49      0.59      2520\n",
            "               Mortgage       0.88      0.75      0.81      2467\n",
            "\n",
            "               accuracy                           0.68     11250\n",
            "              macro avg       0.70      0.69      0.68     11250\n",
            "           weighted avg       0.71      0.68      0.68     11250\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "KNN: \n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Bank account or service       0.74      0.81      0.77      1599\n",
            "            Credit card       0.76      0.76      0.76      2201\n",
            "       Credit reporting       0.67      0.87      0.76      2463\n",
            "        Debt collection       0.83      0.64      0.72      2520\n",
            "               Mortgage       0.93      0.80      0.86      2467\n",
            "\n",
            "               accuracy                           0.78     11250\n",
            "              macro avg       0.79      0.78      0.78     11250\n",
            "           weighted avg       0.79      0.78      0.78     11250\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'scores = cross_val_score(KNN_tfidf, X_train_tfidf, y_train, cv=5)\\nprint(\\'Cross-Validation Accuracy Scores\\', scores)\\nprint(\"KNN mean: \", scores.mean())'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn import model_selection, preprocessing, metrics\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (tf-idf)\n",
        "lr_tfidf = LogisticRegression(solver = 'liblinear', C = 10, penalty = 'l2')\n",
        "lr_tfidf.fit(X_train_tfidf, y_train)  \n",
        "\n",
        "#Predict y value for test dataset\n",
        "y_predict_LR = lr_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "x_predict_LR = lr_tfidf.predict(X_train_tfidf)\n",
        "\n",
        "# Logistic predictions performance\n",
        "print(\"Logistic Regression test accuracy: \\n\")\n",
        "print(classification_report(y_test, y_predict_LR))\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "\"\"\"scores = cross_val_score(lr_tfidf, X_train_tfidf, y_train, cv=5)\n",
        "print('Cross-Validation Accuracy Scores', scores)\n",
        "print(\"Logistic Reg mean: \", scores.mean())\"\"\"\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "#FITTING THE CLASSIFICATION MODEL using SVM (tf-idf)\n",
        "svm_tfidf = SVC(kernel = \"rbf\", C = 1)\n",
        "svm_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "#Predict y value for test dataset\n",
        "y_predict_SVM = svm_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "#x_predict_SVM = svm_tfidf.predict(X_train_tfidf)\n",
        "\n",
        "# SVM predictions performans\n",
        "print(\"SVM Test accuracy: \\n\")\n",
        "print(classification_report(y_test, y_predict_SVM))\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "\"\"\"scores = cross_val_score(svm_tfidf, X_train_tfidf, y_train, cv=5)\n",
        "print('Cross-Validation Accuracy Scores', scores)\n",
        "print(\"SVM mean: \", scores.mean())\"\"\"\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "xgb_model = XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, eta=0.3, silent=1, subsample=0.8)\n",
        "xgb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "xgb_predict = xgb_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "\"\"\"scores = cross_val_score(xgb_model, X_train_tfidf, y_train, cv=5)\n",
        "print('Cross-Validation Accuracy Scores', scores)\n",
        "print(\"XGBoost mean: \", scores.mean())\"\"\"\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "print(\"XGBoost Test accuracy: \\n\")\n",
        "print(classification_report(y_test, xgb_predict))\n",
        "\n",
        "# #FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
        "nb_tfidf = MultinomialNB()\n",
        "nb_tfidf.fit(X_train_tfidf, y_train)  \n",
        "\n",
        "# #Predict y value for test dataset\n",
        "y_predict_nb = nb_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "x_predict_nb = nb_tfidf.predict(X_train_tfidf)\n",
        "\n",
        "# # naive bayes predictions performance\n",
        "print(\"Naive Bayes test accuracy: \\n\")\n",
        "print(classification_report(y_test, y_predict_nb))\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "\"\"\"scores = cross_val_score(nb_tfidf, X_train_tfidf, y_train, cv=5)\n",
        "print('Cross-Validation Accuracy Scores', scores)\n",
        "print(\"NB mean: \", scores.mean())\"\"\"\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "# #FITTING THE CLASSIFICATION MODEL using Random Forest(tf-idf)\n",
        "random_forrest_tfidf = RandomForestClassifier()\n",
        "random_forrest_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_predict_random_forest = random_forrest_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "x_predict_random_forest = random_forrest_tfidf.predict(X_train_tfidf)\n",
        "\n",
        "# # random forest predictions performance\n",
        "print(\"Random Forest test accuracy: \\n\")\n",
        "print(classification_report(y_test, y_predict_random_forest))\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "\"\"\"scores = cross_val_score(random_forrest_tfidf, X_train_tfidf, y_train, cv=5)\n",
        "print('Cross-Validation Accuracy Scores', scores)\n",
        "print(\"Random Forest mean: \", scores.mean())\"\"\"\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "# #FITTING THE CLASSIFICATION MODEL using Decision Tree(tf-idf)\n",
        "decision_tree_tfidf =  tree.DecisionTreeClassifier()\n",
        "decision_tree_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_predict_decision_tree = decision_tree_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "x_predict_decision_tree = decision_tree_tfidf.predict(X_train_tfidf)\n",
        "\n",
        "# # decision tree predictions performance\n",
        "print(\"Decision Tree test accuracy: \\n\")\n",
        "print(classification_report(y_test, y_predict_decision_tree))\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "\"\"\"scores = cross_val_score(decision_tree_tfidf, X_train_tfidf, y_train, cv=5)\n",
        "print('Cross-Validation Accuracy Scores', scores)\n",
        "print(\"Decision Tree mean: \", scores.mean())\"\"\"\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "# #FITTING THE CLASSIFICATION MODEL using Gaussian Naive Bayes(tf-idf)\n",
        "gaussian_nb_tfidf = GaussianNB()\n",
        "gaussian_nb_tfidf.fit(X_train_tfidf.todense(), y_train)  \n",
        "\n",
        "y_predict_gaussian_nb = gaussian_nb_tfidf.predict(X_test_tfidf.todense())\n",
        "\n",
        "# # gaussian naive bayes predictions performance\n",
        "print(\"Gaussian Naive Bayes: \\n\")\n",
        "print(classification_report(y_test, y_predict_gaussian_nb))\n",
        "\n",
        "\"\"\"scores = cross_val_score(gaussian_nb_tfidf, X_train_tfidf, y_train, cv=5)\n",
        "print('Cross-Validation Accuracy Scores', scores)\n",
        "print(\"Gaussian Naive Bayes mean: \", scores.mean())\"\"\"\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "# #FITTING THE CLASSIFICATION MODEL using KNN(tf-idf)\n",
        "KNN_tfidf = KNeighborsClassifier(n_neighbors=4)\n",
        "KNN_tfidf.fit(X_train_tfidf, y_train)  \n",
        "\n",
        "y_predict_KNN = KNN_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "# # KNN predictions performance\n",
        "print(\"KNN: \\n\")\n",
        "print(classification_report(y_test, y_predict_KNN))\n",
        "\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "\n",
        "\"\"\"scores = cross_val_score(KNN_tfidf, X_train_tfidf, y_train, cv=5)\n",
        "print('Cross-Validation Accuracy Scores', scores)\n",
        "print(\"KNN mean: \", scores.mean())\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learning (LSTM)**"
      ],
      "metadata": {
        "id": "QklMIVJBxbHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FgVZy3gQF6LV"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "\n",
        "\n",
        "# deep learning 1\n",
        "\n",
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(x.values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "x = tokenizer.texts_to_sequences(x.values)\n",
        "x = pad_sequences(x, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', x.shape)\n",
        "\n",
        "y = pd.get_dummies(y).values\n",
        "print('Shape of label tensor:', y.shape)\n",
        "\n",
        "#############################################################################################\n",
        "\n",
        "# split train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)\n",
        "\n",
        "#############################################################################################\n",
        "\n",
        "# deep learning 2 \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=x.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "epochs = 8\n",
        "batch_size = 256\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_los', patience=3)])\n",
        "\n",
        "accr = model.evaluate(X_test,y_test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "\n",
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();\n",
        "\n",
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();\n",
        "\n",
        "y_test_predicted = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_test_predicted.round(),target_names= dataset_upsampled_downsampled[\"product\"].unique()))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "consumer_complaint_classification_caretta.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}